{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Coding tutorials\n",
    " #### [1. Saving and loading model weights](#coding_tutorial_1)\n",
    " #### [2. Model saving criteria](#coding_tutorial_2)\n",
    " #### [3. Saving the entire model](#coding_tutorial_3)\n",
    " #### [4. Loading pre-trained Keras models](#coding_tutorial_4)\n",
    " #### [5. Tensorflow Hub modules](#coding_tutorial_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## Saving and loading model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and inspect CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of, in total, 60000 color images, each with one of 10 labels: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. For an introduction and a download, see [this link](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CIFAR-10 dataset and rescale the pixel values\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Use smaller subset -- speeds things up\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 10 CIFAR-10 images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce two useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce function to test model accuracy\n",
    "\n",
    "def get_test_accuracy(model, x_test, y_test):\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce function that creates a new instance of a simple CNN\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "def get_new_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=16, input_shape=(32, 32, 3), kernel_size=(3, 3), \n",
    "               activation='relu', name='conv_1'),\n",
    "        Conv2D(filters=8, kernel_size=(3, 3), activation='relu', name='conv_2'),\n",
    "        MaxPooling2D(pool_size=(4, 4), name='pool_1'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(units=32, activation='relu', name='dense_1'),\n",
    "        Dense(units=10, activation='softmax', name='dense_2')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create simple convolutional neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                12576     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 14,514\n",
      "Trainable params: 14,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model and show model summary\n",
    "\n",
    "model = get_new_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.114\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of the untrained model, around 10% (random)\n",
    "\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object\n",
    "\n",
    "checkpoint_path = 'modelCheckpoint/checkpoint'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                            save_weights_only = True,\n",
    "                            frequency = 'epoch',\n",
    "                            verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples\n",
      "Epoch 1/3\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.9556 - accuracy: 0.2877\n",
      "Epoch 00001: saving model to modelCheckpoint/checkpoint\n",
      "10000/10000 [==============================] - 48s 5ms/sample - loss: 1.9553 - accuracy: 0.2878\n",
      "Epoch 2/3\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.5752 - accuracy: 0.4346\n",
      "Epoch 00002: saving model to modelCheckpoint/checkpoint\n",
      "10000/10000 [==============================] - 46s 5ms/sample - loss: 1.5747 - accuracy: 0.4348\n",
      "Epoch 3/3\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.4589 - accuracy: 0.4800\n",
      "Epoch 00003: saving model to modelCheckpoint/checkpoint\n",
      "10000/10000 [==============================] - 46s 5ms/sample - loss: 1.4587 - accuracy: 0.4801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f29cc5d4630>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model, with simple checkpoint which saves (and overwrites) model weights every epoch\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 3, callbacks= [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 184K\r\n",
      "-rw-r--r-- 1 jovyan users   77 Nov 30 12:55 checkpoint\r\n",
      "-rw-r--r-- 1 jovyan users 174K Nov 30 12:55 checkpoint.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Nov 30 12:55 checkpoint.index\r\n"
     ]
    }
   ],
   "source": [
    "# Have a look at what the checkpoint creates\n",
    "\n",
    "! ls -lh modelCheckpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.470\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the trained model\n",
    "\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new model, load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.129\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the (initialised) model, accuracy around 10% again\n",
    "\n",
    "model = get_new_model()\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights -- accuracy is the same as the trained model\n",
    "\n",
    "model.load_weights(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Model saving criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create more customised checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object with epoch and batch details\n",
    "\n",
    "checkpoint_5000_path = 'modelCheckpoint_5000/checkpoint_{epoch:02d}_{batch:04d}'\n",
    "\n",
    "checkpoint_5000 = ModelCheckpoint(filepath= checkpoint_5000_path,\n",
    "                                 save_weights_only= True,\n",
    "                                 save_freq= 5000,\n",
    "                                 verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      " 4990/10000 [=============>................] - ETA: 30s - loss: 2.0972 - accuracy: 0.2301\n",
      "Epoch 00001: saving model to modelCheckpoint_5000/checkpoint_01_0499\n",
      " 9990/10000 [============================>.] - ETA: 0s - loss: 1.9117 - accuracy: 0.2969\n",
      "Epoch 00001: saving model to modelCheckpoint_5000/checkpoint_01_0999\n",
      "10000/10000 [==============================] - 62s 6ms/sample - loss: 1.9113 - accuracy: 0.2969 - val_loss: 1.6259 - val_accuracy: 0.4270\n",
      "Epoch 2/3\n",
      " 4990/10000 [=============>................] - ETA: 29s - loss: 1.5531 - accuracy: 0.4393\n",
      "Epoch 00002: saving model to modelCheckpoint_5000/checkpoint_02_0499\n",
      " 9980/10000 [============================>.] - ETA: 0s - loss: 1.5213 - accuracy: 0.4540\n",
      "Epoch 00002: saving model to modelCheckpoint_5000/checkpoint_02_0999\n",
      "10000/10000 [==============================] - 60s 6ms/sample - loss: 1.5209 - accuracy: 0.4543 - val_loss: 1.4396 - val_accuracy: 0.4870\n",
      "Epoch 3/3\n",
      " 4990/10000 [=============>................] - ETA: 29s - loss: 1.3878 - accuracy: 0.5118\n",
      "Epoch 00003: saving model to modelCheckpoint_5000/checkpoint_03_0499\n",
      " 9980/10000 [============================>.] - ETA: 0s - loss: 1.3853 - accuracy: 0.5074\n",
      "Epoch 00003: saving model to modelCheckpoint_5000/checkpoint_03_0999\n",
      "10000/10000 [==============================] - 60s 6ms/sample - loss: 1.3850 - accuracy: 0.5076 - val_loss: 1.3987 - val_accuracy: 0.5030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa10da87d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit model with checkpoint\n",
    "\n",
    "model = get_new_model()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         validation_data = (x_test, y_test),\n",
    "         epochs = 3,\n",
    "         batch_size = 10,verbose = 1,\n",
    "         callbacks = [checkpoint_5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.1M\r\n",
      "-rw-r--r-- 1 jovyan users   93 Dec  2 12:02 checkpoint\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:00 checkpoint_01_0499.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:00 checkpoint_01_0499.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:00 checkpoint_01_0999.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:00 checkpoint_01_0999.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:01 checkpoint_02_0499.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:01 checkpoint_02_0499.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:01 checkpoint_02_0999.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:01 checkpoint_02_0999.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:02 checkpoint_03_0499.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:02 checkpoint_03_0499.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:02 checkpoint_03_0999.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:02 checkpoint_03_0999.index\r\n"
     ]
    }
   ],
   "source": [
    "# Have a look at what the checkpoint creates\n",
    "! ls -lh modelCheckpoint_5000/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with model saving criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tiny training and test set -- will overfit!\n",
    "\n",
    "x_train = x_train[:100]\n",
    "y_train = y_train[:100]\n",
    "x_test = x_test[:100]\n",
    "y_test = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of untrained model\n",
    "\n",
    "model = get_new_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object which monitors the validation accuracy\n",
    "\n",
    "checkpoint_best_path = 'modelcheckpoint_best/checkpoint'\n",
    "\n",
    "checkpoint_best = ModelCheckpoint(filepath= checkpoint_best_path,\n",
    "                                 save_weights_only=True,\n",
    "                                 save_best_only= True,\n",
    "                                 monitor='val_accuracy',\n",
    "                                 save_freq = 'epoch',\n",
    "                                 verbose =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.06000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.06000\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.06000\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.06000 to 0.09000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.09000 to 0.11000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.11000\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.11000\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.11000\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.11000\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.11000 to 0.15000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.15000\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.15000\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.15000\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.15000 to 0.19000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.19000\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.19000 to 0.20000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.20000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.20000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.20000\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.20000\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.20000\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.20000\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.20000 to 0.22000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.22000\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.22000 to 0.24000, saving model to modelcheckpoint_best/checkpoint\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.24000\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.24000\n"
     ]
    }
   ],
   "source": [
    "# Fit the model and save only the weights with the highest validation accuracy\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 50,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    batch_size = 10,\n",
    "                    verbose = 0,\n",
    "                    callbacks= [checkpoint_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa1ae8a5668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and testing curves\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(y=['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 184K\r\n",
      "-rw-r--r-- 1 jovyan users   77 Dec  2 12:27 checkpoint\r\n",
      "-rw-r--r-- 1 jovyan users 174K Dec  2 12:27 checkpoint.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K Dec  2 12:27 checkpoint.index\r\n"
     ]
    }
   ],
   "source": [
    "# Inspect the checkpoint directory\n",
    "\n",
    "! ls -lh modelcheckpoint_best/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.240\n"
     ]
    }
   ],
   "source": [
    "# Create a new model with the saved weights\n",
    "\n",
    "model = get_new_model()\n",
    "\n",
    "model.load_weights(checkpoint_best_path)\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'model_checkpoints_5000': No such file or directory\r\n",
      "rm: cannot remove 'model_checkpoints_best': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! rm -r model_checkpoints_5000 model_checkpoints_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## Saving the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create checkpoint that saves whole model, not just weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit model with checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect what the checkpoint has created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at what the checkpoint creates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter variables directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's test accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the .h5 format to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in .h5 format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect .h5 file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r model_checkpoints\n",
    "! rm my_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## Loading pre-trained Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and build Keras ResNet50 model\n",
    "\n",
    "Today we'll be using the ResNet50 model designed by a team at Microsoft Research, available through Keras applications. Please see the description on the [Keras applications page](https://keras.io/applications/#resnet) for details. If you continue using it, please cite it properly! The paper it comes from is:\n",
    "\n",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. \"Deep Residual Learning for Image Recognition\", 2015.\n",
    "\n",
    "This model takes a long time to download on the Coursera platform, so it is pre-downloaded in your workspace and saved in Keras HDF5 format. If you want to import it on your personal machine, use the following code:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "model = ResNet50(weights='imagenet')\n",
    "```\n",
    "\n",
    "In this coding tutorial, you will instead load the model directly from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Keras ResNet50 model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and preprocess 3 sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3 sample ImageNet images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "lemon_img = load_img('data/lemon.jpg', target_size=(224, 224))\n",
    "viaduct_img = load_img('data/viaduct.jpg', target_size=(224, 224))\n",
    "water_tower_img = load_img('data/water_tower.jpg', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ResNet50 model to classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: presents top 5 predictions and probabilities\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_5_predictions(img):\n",
    "    x = img_to_array(img)[np.newaxis, ...]\n",
    "    x = preprocess_input(x)\n",
    "    preds = decode_predictions(model.predict(x), top=5)\n",
    "    top_preds = pd.DataFrame(columns=['prediction', 'probability'],\n",
    "                             index=np.arange(5)+1)\n",
    "    for i in range(5):\n",
    "        top_preds.loc[i+1, 'prediction'] = preds[0][i][1]\n",
    "        top_preds.loc[i+1, 'probability'] = preds[0][i][2] \n",
    "    return top_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 1: lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 2: viaduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 3: water tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Tensorflow Hub modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and build Tensorflow Hub MobileNet v1 model\n",
    "\n",
    "Today we'll be using Google's MobileNet v1 model, available on Tensorflow Hub. Please see the description on the [Tensorflow Hub page](https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/classification/4) for details on it's architecture, how it's trained, and the reference. If you continue using it, please cite it properly! The paper it comes from is:\n",
    "\n",
    "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam: \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\", 2017.\n",
    "\n",
    "This model takes a long time to download on the Coursera platform, so it is pre-downloaded in your workspace and saved in Tensorflow SavedModel format. If you want to import it on your personal machine, use the following code:\n",
    "\n",
    "```python\n",
    "module_url = \"https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/classification/4\"\n",
    "model = Sequential([hub.KerasLayer(module_url)])\n",
    "model.build(input_shape=[None, 160, 160, 3])\n",
    "```\n",
    "\n",
    "In this coding tutorial, you will instead load the model directly from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Google's Mobilenet v1 model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use MobileNet model to classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and preprocess 3 sample ImageNet images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "lemon_img = load_img(\"data/lemon.jpg\", target_size=(160, 160))\n",
    "viaduct_img = load_img(\"data/viaduct.jpg\", target_size=(160, 160))\n",
    "water_tower_img = load_img(\"data/water_tower.jpg\", target_size=(160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in categories text file\n",
    "\n",
    "with open('data/imagenet_categories.txt') as txt_file:\n",
    "    categories = txt_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: presents top 5 predictions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_5_predictions(img):\n",
    "    x = img_to_array(img)[np.newaxis, ...] / 255.0\n",
    "    preds = model.predict(x)\n",
    "    top_preds = pd.DataFrame(columns=['prediction'],\n",
    "                             index=np.arange(5)+1)\n",
    "    sorted_index = np.argsort(-preds[0])\n",
    "    for i in range(5):\n",
    "        ith_pred = categories[sorted_index[i]]\n",
    "        top_preds.loc[i+1, 'prediction'] = ith_pred\n",
    "            \n",
    "    return top_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 1: lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 2: viaduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 3: water tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
